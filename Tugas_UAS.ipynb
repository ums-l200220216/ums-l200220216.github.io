{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d81a1479-6b14-4326-adc6-89e8d9369713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data telah disimpan ke file: data_group.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Baca file teks hasil eksport\n",
    "file_path = \"data_group.txt\"  # Ubah dengan lokasi file Anda\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Regex untuk memisahkan tanggal, waktu, pengirim, dan pesan\n",
    "pattern = r\"(\\d{1,2}/\\d{1,2}/\\d{2,4}), (\\d{1,2}[:.\\d]{1,5})\\s?- ([^:]+): (.+)\"\n",
    "\n",
    "# Parsing data\n",
    "data = []\n",
    "for line in lines:\n",
    "    match = re.match(pattern, line)\n",
    "    if match:\n",
    "        date, time, sender, message = match.groups()\n",
    "        data.append({\"Date\": date, \"Time\": time, \"Sender\": sender, \"Message\": message})\n",
    "\n",
    "# Buat DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Simpan ke file CSV\n",
    "csv_file = \"data_group.csv\"\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(f\"Data telah disimpan ke file: {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca325723-af23-4786-b29b-b0c7d0a21b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beberapa baris pertama dari file yang diekstrak:\n",
      "﻿Tanggal;Waktu;Pengirim;Pesan\n",
      "10/09/24;5:27 pm;Mas Putra Guard;_*PAM BOLA LIGA 1*_\n",
      "10/09/24;5:27 pm;+62 821-3760-6858;_*PAM BOLA LIGA 1*_\n",
      "10/09/24;5:27 pm;+62 831-0471-1145;_*PAM BOLA LIGA 1*_\n",
      "10/09/24;5:27 pm;+62 814-6976-8009;_*PAM BOLA LIGA 1*_\n",
      "10/09/24;5:28 pm;Mas Putra Guard;Wait tak rapikan dulu, pelan²\n",
      "10/09/24;5:28 pm;Mas Putra Guard;_*PAM BOLA LIGA 1*_\n",
      "10/09/24;5:28 pm;Mas Putra Guard;Monggo lanjutkan\n",
      "10/09/24;5:29 pm;+62 821-4434-4590;_*PAM BOLA LIGA 1*_\n",
      "10/09/24;5:29 pm;+62 8\n",
      "Terjadi kesalahan: Error tokenizing data. C error: Expected 1 fields in line 6, saw 2\n",
      "\n",
      "File sementara data_group_cleaned.csv tidak ditemukan untuk dihapus.\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import pandas as pd\n",
    "import re\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "# Fungsi untuk membaca file dari arsip .tar\n",
    "def load_file_from_tar(tar_path, file_name):\n",
    "    with tarfile.open(tar_path, 'r') as tar:\n",
    "        # Mencari file di dalam arsip\n",
    "        file = tar.extractfile(file_name)\n",
    "        if file:\n",
    "            return file.read().decode('utf-8')  # Mengembalikan data sebagai string\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"{file_name} tidak ditemukan dalam arsip.\")\n",
    "\n",
    "# Tentukan path arsip .tar dan nama file di dalam arsip\n",
    "tar_file_path = \"data_group.tar\"\n",
    "file_name_in_tar = \"data_group.csv\"  # Nama file di dalam arsip\n",
    "output_csv_path = \"data_group_cleaned.csv\"  # Lokasi file output sementara\n",
    "\n",
    "# Membaca data dari arsip .tar\n",
    "try:\n",
    "    file_content = load_file_from_tar(tar_file_path, file_name_in_tar)\n",
    "    \n",
    "    # Tampilkan beberapa baris pertama dari file untuk debugging\n",
    "    print(\"Beberapa baris pertama dari file yang diekstrak:\")\n",
    "    print(file_content[:500])  # Menampilkan 500 karakter pertama untuk pengecekan\n",
    "    \n",
    "    # Membaca data sebagai CSV\n",
    "    file_data = StringIO(file_content)\n",
    "    df = pd.read_csv(file_data)\n",
    "\n",
    "    # Debug: Cek apakah kolom 'Message' ada\n",
    "    if 'Message' not in df.columns:\n",
    "        print(\"Kolom 'Message' tidak ditemukan!\")\n",
    "        print(\"Kolom yang tersedia:\", df.columns)\n",
    "    else:\n",
    "        # Fungsi untuk membersihkan teks\n",
    "        def clean_text(text):\n",
    "            if isinstance(text, str):\n",
    "                # Hanya mempertahankan angka, huruf, dan tanda baca-tulis umum\n",
    "                return re.sub(r'[^a-zA-Z0-9.,:;!?()\\-\"\\' ]', '', text)\n",
    "            return text\n",
    "        \n",
    "        # Bersihkan kolom 'Message'\n",
    "        df['Message'] = df['Message'].apply(clean_text)\n",
    "        \n",
    "        # Simpan hasil ke file CSV sementara\n",
    "        df.to_csv(output_csv_path, index=False, encoding=\"utf-8\")\n",
    "        print(f\"Data telah dibersihkan dan disimpan sementara ke file: {output_csv_path}\")\n",
    "\n",
    "        # Perbarui arsip .tar dengan file CSV hasil\n",
    "        with tarfile.open(tar_file_path, \"a\") as tar:  # Gunakan mode \"a\" untuk menambahkan ke arsip yang ada\n",
    "            tar.add(output_csv_path, arcname=\"data_group_cleaned.csv\")\n",
    "            print(f\"File hasil telah ditambahkan ke arsip: {tar_file_path}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Terjadi kesalahan: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan: {e}\")\n",
    "finally:\n",
    "    # Hapus file sementara setelah digunakan\n",
    "    if os.path.exists(output_csv_path):\n",
    "        os.remove(output_csv_path)\n",
    "        print(f\"File sementara {output_csv_path} telah dihapus.\")\n",
    "    else:\n",
    "        print(f\"File sementara {output_csv_path} tidak ditemukan untuk dihapus.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53cd83e-7dc7-4eef-ba39-e37020ecf0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (100, 9)\n",
      "Feature names: ['bola', 'liga', 'media', 'mohon', 'ndan', 'omitted', 'pam', 'siap', 'yang']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def load_yelp_reviews(file_path, num_docs):\n",
    "    # Membaca file CSV ke dalam DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Memastikan kolom 'Pesan' ada\n",
    "    if 'Pesan' not in df.columns:\n",
    "        raise KeyError(\"Kolom 'Pesan' tidak ditemukan dalam file.\")\n",
    "    \n",
    "    # Hilangkan baris dengan NaN di kolom 'Pesan'\n",
    "    df['Pesan'] = df['Pesan'].fillna(\"\").astype(str)\n",
    "    \n",
    "    # Mengambil sejumlah dokumen sesuai num_docs\n",
    "    return list(islice(df['Pesan'], num_docs))\n",
    "\n",
    "def make_matrix(docs, binary=False):\n",
    "    # Modify min_df and max_df values\n",
    "    vec = CountVectorizer(min_df=5, max_df=0.9, binary=binary)  # Adjusted parameters\n",
    "    mtx = vec.fit_transform(docs)\n",
    "    \n",
    "    # Extract column names\n",
    "    cols = [None] * len(vec.vocabulary_)\n",
    "    for word, index in vec.vocabulary_.items():\n",
    "        cols[index] = word\n",
    "    \n",
    "    return mtx, cols\n",
    "\n",
    "# Path file CSV yang diunggah\n",
    "file_path = \"data_group_cleaned.csv\"\n",
    "\n",
    "# Contoh penggunaan\n",
    "num_docs = 100\n",
    "try:\n",
    "    docs = load_yelp_reviews(file_path, num_docs)\n",
    "    mtx, cols = make_matrix(docs)\n",
    "    \n",
    "    print(\"Matrix shape:\", mtx.shape)\n",
    "    print(\"Feature names:\", cols[:10])  # Menampilkan 10 fitur pertama\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f872a29-e5a4-4f79-9ea4-ac76aa6c11ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Matrix disimpan ke file: data_group.tar\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "tar_file_path = \"data_group.tar\"\n",
    "\n",
    "scipy.sparse.save_npz('matrix.npz', mtx)\n",
    "pd.DataFrame({'feature': cols}).to_csv('features.csv', index=False)\n",
    "print(f\"Data Matrix disimpan ke file: {tar_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "729ca121-6ad7-48bf-917b-c2998de1001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "import numpy as np\n",
    "\n",
    "def top_words(num_clusters, clusters, mtx, columns):\n",
    "    top = []\n",
    "    for i in range(num_clusters):  # Loop over each cluster\n",
    "        rows_in_cluster = np.where(clusters == i)[0]  # Get rows belonging to the current cluster\n",
    "        word_freqs = mtx[rows_in_cluster].sum(axis=0).A[0]  # Sum word frequencies for the cluster\n",
    "        ordered_freqs = np.argsort(word_freqs)  # Sort frequencies\n",
    "        top_words = [\n",
    "            (columns[idx], int(word_freqs[idx]))  # Get top words and their frequencies\n",
    "            for idx in islice(reversed(ordered_freqs), 20)\n",
    "        ]\n",
    "        top.append(top_words)\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "094fd782-fbd5-4c1d-887a-34e78482b2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriks fitur dibuat dengan shape: (100, 320)\n",
      "Contoh kolom fitur: ['0005', '0009', '0031', '0075', '01', '0373', '0471', '0796', '08', '0960']\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fungsi untuk membaca file dari arsip .tar\n",
    "def load_custom_data(file_path, file_name, num_docs):\n",
    "    with tarfile.open(file_path, 'r') as tar:\n",
    "        # Membuka file tertentu di dalam arsip\n",
    "        datafile = tar.extractfile(file_name)\n",
    "        if datafile:\n",
    "            # Mengembalikan baris pertama sejumlah num_docs\n",
    "            return list(islice(datafile, num_docs))\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"{file_name} tidak ditemukan dalam arsip.\")\n",
    "\n",
    "# Fungsi untuk membuat matriks fitur dari teks\n",
    "def make_matrix(docs, binary=False):\n",
    "    vec = CountVectorizer(min_df=1, max_df=0.9, binary=binary)\n",
    "    mtx = vec.fit_transform(docs)\n",
    "    cols = [None] * len(vec.vocabulary_)\n",
    "    for word, idx in vec.vocabulary_.items():\n",
    "        cols[idx] = word\n",
    "    return mtx, cols\n",
    "\n",
    "file_path = 'data_group.tar'\n",
    "file_name = 'data_group.csv'\n",
    "num_docs = 100 \n",
    "\n",
    "# Membaca data\n",
    "try:\n",
    "    docs = load_custom_data(file_path, file_name, num_docs)\n",
    "    # Mengubah byte ke string (jika diperlukan)\n",
    "    docs = [doc.decode('utf-8').strip() for doc in docs]\n",
    "\n",
    "    # Membuat matriks fitur\n",
    "    matrix, columns = make_matrix(docs)\n",
    "    print(\"Matriks fitur dibuat dengan shape:\", matrix.shape)\n",
    "    print(\"Contoh kolom fitur:\", columns[:10])\n",
    "except Exception as e:\n",
    "    print(\"Terjadi kesalahan:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361dc665-e324-4231-9282-a6dbefa9d796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mengekstrak data_group_cleaned.csv dari data_group.tar...\n",
      "Data dengan kluster telah disimpan ke file: data_group_clustered.csv\n",
      "File data_group_clustered.csv telah ditambahkan ke arsip data_group.tar.\n",
      "File sementara data_group_clustered.csv telah dihapus.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Fungsi untuk membaca file dari arsip .tar\n",
    "def extract_csv_from_tar(tar_path, file_name):\n",
    "    with tarfile.open(tar_path, 'r') as tar:\n",
    "        # Memeriksa apakah file yang dimaksud ada di dalam arsip\n",
    "        if file_name in tar.getnames():\n",
    "            print(f\"Mengekstrak {file_name} dari {tar_path}...\")\n",
    "            extracted_file = tar.extractfile(file_name)\n",
    "            if extracted_file:\n",
    "                return pd.read_csv(extracted_file)\n",
    "        else:\n",
    "            print(f\"File {file_name} tidak ditemukan di arsip. Berikut file yang tersedia: {tar.getnames()}\")\n",
    "            raise FileNotFoundError(f\"{file_name} tidak ditemukan di dalam arsip {tar_path}.\")\n",
    "\n",
    "# Fungsi untuk menambahkan file ke arsip tar\n",
    "def add_file_to_tar(tar_path, file_to_add):\n",
    "    with tarfile.open(tar_path, 'a') as tar:  # Open in append mode\n",
    "        tar.add(file_to_add, arcname=os.path.basename(file_to_add))\n",
    "        print(f\"File {file_to_add} telah ditambahkan ke arsip {tar_path}.\")\n",
    "\n",
    "# Path ke file tar dan nama file CSV di dalamnya\n",
    "tar_path = 'data_group.tar'\n",
    "file_name = 'data_group_cleaned.csv'\n",
    "\n",
    "# Membaca file CSV dari arsip .tar\n",
    "try:\n",
    "    data = extract_csv_from_tar(tar_path, file_name)\n",
    "    if data is None or data.empty:\n",
    "        raise ValueError(\"Dataset kosong atau tidak berhasil dimuat!\")\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Pastikan kolom 'Pesan' ada\n",
    "if 'Pesan' not in data.columns:\n",
    "    raise KeyError(\"Kolom 'Pesan' tidak ditemukan di dataset!\")\n",
    "\n",
    "# Step 1: Preprocess the text data (cleaning the \"Pesan\" column)\n",
    "data['Pesan'] = data['Pesan'].fillna(\"\").str.lower()  # Fill NaN and convert to lowercase\n",
    "\n",
    "# Step 2: Convert the \"Pesan\" column into numerical representation using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(data['Pesan'])\n",
    "\n",
    "# Step 3: Perform KMeans clustering with 3, 4, and 5 clusters\n",
    "clusters = {}\n",
    "for n_clusters in [3, 4, 5]:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters[n_clusters] = kmeans.fit_predict(X)\n",
    "\n",
    "# Add cluster labels to the dataset for each KMeans run\n",
    "for n_clusters, labels in clusters.items():\n",
    "    data[f'Cluster_{n_clusters}'] = labels\n",
    "\n",
    "# Save the updated dataset to a new CSV file\n",
    "output_file = 'data_group_clustered.csv'\n",
    "data.to_csv(output_file, index=False)\n",
    "print(f\"Data dengan kluster telah disimpan ke file: {output_file}\")\n",
    "\n",
    "# Step 4: Add the resulting CSV file to the tar archive\n",
    "try:\n",
    "    add_file_to_tar(tar_path, output_file)\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan saat menambahkan file ke arsip: {e}\")\n",
    "\n",
    "# Hapus file sementara setelah digunakan\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "    print(f\"File sementara {output_file} telah dihapus.\")\n",
    "else:\n",
    "    print(f\"File sementara {output_file} tidak ditemukan untuk dihapus.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ccf72d1-5d24-454d-b7e1-df604516cc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mengekstrak data_group_clustered.csv dari data_group.tar...\n",
      "Kluster 0: [('mas', 839), ('media', 521), ('omitted', 521)]\n",
      "Kluster 1: [('siap', 956), ('ndan', 888), ('mas', 34)]\n",
      "Kluster 2: [('pam', 652), ('bola', 652), ('liga', 156)]\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Fungsi untuk membaca file dari arsip .tar\n",
    "def extract_csv_from_tar(tar_path, file_name):\n",
    "    with tarfile.open(tar_path, 'r') as tar:\n",
    "        # Memeriksa apakah file yang dimaksud ada di dalam arsip\n",
    "        if file_name in tar.getnames():\n",
    "            print(f\"Mengekstrak {file_name} dari {tar_path}...\")\n",
    "            extracted_file = tar.extractfile(file_name)\n",
    "            if extracted_file:\n",
    "                return pd.read_csv(extracted_file)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"{file_name} tidak ditemukan di dalam arsip {tar_path}.\")\n",
    "\n",
    "# Path ke file tar dan nama file CSV di dalamnya\n",
    "tar_path = 'data_group.tar'\n",
    "file_name = 'data_group_clustered.csv'\n",
    "\n",
    "# Membaca file CSV dari arsip .tar\n",
    "try:\n",
    "    data = extract_csv_from_tar(tar_path, file_name)\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Pastikan kolom 'Pesan' dan 'Cluster_3' ada\n",
    "if 'Pesan' not in data.columns or 'Cluster_3' not in data.columns:\n",
    "    raise KeyError(\"Kolom 'Pesan' atau 'Cluster_3' tidak ditemukan di dataset!\")\n",
    "\n",
    "# Mendapatkan 3 kata teratas untuk setiap kluster\n",
    "for cluster_id in sorted(data['Cluster_3'].unique()):  # Iterasi berdasarkan kluster\n",
    "    # Mengambil pesan-pesan dari kluster tertentu\n",
    "    cluster_messages = data[data['Cluster_3'] == cluster_id]['Pesan']\n",
    "    \n",
    "    # Mengonversi pesan menjadi string dan mengganti NaN dengan string kosong\n",
    "    cluster_messages = cluster_messages.fillna(\"\").astype(str)\n",
    "    \n",
    "    # Menggabungkan semua pesan dalam kluster menjadi satu string dan memisahkannya menjadi kata-kata\n",
    "    all_words = \" \".join(cluster_messages).split()\n",
    "    \n",
    "    # Mendapatkan 3 kata teratas menggunakan Counter\n",
    "    most_common_words = Counter(all_words).most_common(3)\n",
    "    \n",
    "    # Menampilkan hasil\n",
    "    print(f\"Kluster {cluster_id}: {most_common_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1d9686b-171c-42d7-b57a-4f5311e048fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil analisis telah disimpan ke cluster_analysis.csv\n",
      "   Cluster                           Top Words\n",
      "0        2   pam (652), bola (652), liga (156)\n",
      "1        0  mas (962), ndan (591), media (521)\n",
      "2        1    siap (962), ndan (908), mas (35)\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Fungsi untuk membaca file hasil klastering dari tar\n",
    "def read_clustered_data_from_tar(tar_file_path, csv_file_name):\n",
    "    with tarfile.open(tar_file_path, \"r\") as tar:\n",
    "        extracted_file = tar.extractfile(csv_file_name)\n",
    "        if extracted_file:\n",
    "            df = pd.read_csv(extracted_file)\n",
    "            return df\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"{csv_file_name} not found in {tar_file_path}\")\n",
    "\n",
    "# Fungsi untuk mendapatkan 3 kata teratas dari setiap kluster\n",
    "def analyze_clusters(data, cluster_column, message_column):\n",
    "    analysis = []\n",
    "    for cluster_id in data[cluster_column].unique():\n",
    "        cluster_messages = data[data[cluster_column] == cluster_id][message_column].fillna(\"\").astype(str)\n",
    "        all_words = \" \".join(cluster_messages).split()\n",
    "        clean_words = [re.sub(r'[^a-zA-Z0-9]', '', word).lower() for word in all_words if len(word) > 1]\n",
    "        most_common_words = Counter(clean_words).most_common(3)\n",
    "        analysis.append({\n",
    "            \"Cluster\": cluster_id,\n",
    "            \"Top Words\": \", \".join([f\"{word} ({count})\" for word, count in most_common_words])\n",
    "        })\n",
    "    return pd.DataFrame(analysis)\n",
    "\n",
    "# File TAR dan nama file hasil klastering di dalamnya\n",
    "tar_file_path = \"data_group.tar\"\n",
    "clustered_file_name = \"data_group_clustered.csv\"\n",
    "\n",
    "# Analisis data hasil klastering\n",
    "try:\n",
    "    df = read_clustered_data_from_tar(tar_file_path, clustered_file_name)\n",
    "    cluster_analysis = analyze_clusters(df, cluster_column=\"Cluster_3\", message_column=\"Pesan\")\n",
    "\n",
    "    # Simpan hasil analisis ke CSV\n",
    "    analysis_csv_path = \"cluster_analysis.csv\"\n",
    "    cluster_analysis.to_csv(analysis_csv_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Hasil analisis telah disimpan ke {analysis_csv_path}\")\n",
    "    print(cluster_analysis)\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db51fad-074a-4ce1-a18c-4526d4eceeba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
